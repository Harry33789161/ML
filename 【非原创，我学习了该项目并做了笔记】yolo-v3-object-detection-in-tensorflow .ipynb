{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # 【非原创，我学习了项目并加上了笔记】Yolo v3 Object Detection in Tensorflow\n [GitHub repo link](https://github.com/heartkilla/yolo-v3) <br>\n UPDATE: Video processing\n <a id=\"top\"></a> <br>\n## 对做此笔记的解释\n我做笔记的动机是为了在有限的时间里（尽可能）深入了解一类模型，以及了解（该领域中）的代码风格。\n\n因为我想在保留原文的前提下，不让笔记和原文混在一起，同时简化操作，所以在保留原文的基础上用中文记笔记。\n\n我尽量搜索/总结最靠谱的结论，有错误欢迎批评\n\n想阅读纯净原版请移步https://www.kaggle.com/code/aruchomu/yolo-v3-object-detection-in-tensorflow\n\n## Content\n1. [What is Yolo?](#1)\n2. [Dependencies](#2)\n3. [Model hyperparameters](#3)\n4. [Model definiton](#4)\n5. [Utility functions](#5)\n6. [Converting weights to Tensorflow format](#6)\n7. [Running the model](#7)\n8. [Video processing](#8)\n9. [To-Do list](#9)\n10. [Acknowledgements](#10)","metadata":{"_uuid":"caea89c96714929c43ae69734c83131551e4dfa2"}},{"cell_type":"markdown","source":"<a id=\"1\"></a> \n## 1. What is Yolo?\nYolo is an algorithm that uses convolutional neural networks for object detection. <br>\nSo what's great about object detection? In comparison to recognition algorithms, a detection algorithm does not only predict class labels, but detects locations of objects as well. \n### Yolo网络结构图\n![](https://img-blog.csdnimg.cn/2019040211084050.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70)","metadata":{"_uuid":"2e0fd736773c7063f7566f9e1a3dd5c2e1514012"}},{"cell_type":"markdown","source":"<a id=\"2\"></a> \n## 2. Dependencies\nTo build Yolo we're going to need Tensorflow (deep learning), NumPy (numerical computation) and Pillow (image processing) libraries. Also we're going to use seaborn's color palette for bounding boxes colors. Finally, let's import IPython function `display()` to display images in the notebook.","metadata":{"_uuid":"74c2b95e06bf3dfda575d737936438fb983b60cf"}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\nfrom IPython.display import display\nfrom seaborn import color_palette\nimport cv2\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-21T11:58:24.719605Z","iopub.execute_input":"2022-06-21T11:58:24.720262Z","iopub.status.idle":"2022-06-21T11:58:28.581027Z","shell.execute_reply.started":"2022-06-21T11:58:24.720189Z","shell.execute_reply":"2022-06-21T11:58:28.580087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n## 3. Model hyperparameters\nNext, we define some configurations for Yolo. ","metadata":{"_uuid":"2a30a24e3cc58704b009806e1c6efb1f7ca778f6"}},{"cell_type":"code","source":"_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-05\n_LEAKY_RELU = 0.1\n_ANCHORS = [(10, 13), (16, 30), (33, 23),\n            (30, 61), (62, 45), (59, 119),\n            (116, 90), (156, 198), (373, 326)]\n_MODEL_SIZE = (416, 416)","metadata":{"_uuid":"8c963b6244cf03abad2defc60e7bb0b5dde7cfda","execution":{"iopub.status.busy":"2022-06-21T11:58:28.583378Z","iopub.execute_input":"2022-06-21T11:58:28.583772Z","iopub.status.idle":"2022-06-21T11:58:28.590715Z","shell.execute_reply.started":"2022-06-21T11:58:28.583713Z","shell.execute_reply":"2022-06-21T11:58:28.589475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`_MODEL_SIZE` refers to the input size of the model.\n\nLet's look at other parameters step-by-step.","metadata":{"_uuid":"3e88ca967006cf686c53568071030cb968359716"}},{"cell_type":"markdown","source":"### 对_BATCH_NORM_EPSILON 和 _BATCH_NORM_EPSILON 两个参数的解释\n在深度学习网络训练过程中，由于数据集量级较大，常常把数据集分批次送入网络训练，那么在BN层计算均值和方差的时候，就没办法一次性计算出整个数据集的均值和方差，在mini batch训练阶段并无太大影响，因为mini batch训练时的方差和均值就是根据mini batch内的数据计算出的，但是当模型用于推理时，使用的均值和方差是训练集的均值和方差，于是需要一个方法来计算数据集的均值和方差，通常使用滑动平均法(momentum一般为0.1)计算全局均值和方差：\n训练和推理阶段的区别：\n\n训练阶段根据mini batch的数据计算均值和方差，并使用滑动平均法计算全局均值和方差；推理阶段使用训练阶段计算的全局均值和方差参与计算。\n————————————————\n版权声明：本文为CSDN博主「嘟嘟太菜了」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://blog.csdn.net/qq_40035462/article/details/123371566\n### Batch normalization\nAlmost every convolutional layer in Yolo has batch normalization after it. It helps the model train faster and reduces variance between units (and total variance as well). Batch normalization is defined as follows.\n<br>\n![](https://hsto.org/files/005/d19/2bd/005d192bd6274c298f75896498aea377.png)\n<br>\n`_BATCH_NORM_EPSILON` refers to epsilon in this formula, whereas `_BATCH_NORM_DECAY` refers to momentum, which is used for computing moving average and variance. We use them in forward propagation during inference (after training).\n<br>\n<br>\n`moving_average = momentum * moving_average + (1 - momentum) * current_average`","metadata":{"_uuid":"c0292f7ea55aecc64c24cd4b7d4ca0c3292abcb2"}},{"cell_type":"markdown","source":"### 对_LEAKY_RELU参数的解释\n为了不让神经元完全死亡\n### Leaky ReLU¶\nLeaky ReLU is a slight modification of ReLU activation function. The idea behind Leaky ReLU is to prevent so-called \"neuron dying\" when a large number of activations become 0.","metadata":{"_uuid":"bf8f4099862db8e75795f70e368534b5011800de"}},{"cell_type":"markdown","source":"### 对此段的理解\n在训练好的初始锚点的基础上，预测新的锚点位置，Sigmoid函数保证了bbox中心不会偏移到分格的外面。因为特征图和原图尺寸不一样，所以要乘系数$P_w,P_h$\n### Anchors\nAnchors are sort of bounding box priors, that were calculated on the COCO dataset using k-means clustering. We are going to predict the width and height of the box as offsets from cluster centroids. The center coordinates of the box relative to the location of filter application are predicted using a sigmoid function.\n<br>\n$$b_{x} = \\sigma(t_{x})+c_{x}$$\n$$b_{y} = \\sigma(t_{y})+c_{y}$$\n$$b_{w} = p_{w}e^{t_{w}}$$\n$$b_{h} = p_{h}e^{t_{h}}$$\n<br>\nWhere $b_{x}$ and $b_{y}$ are the center coordinates of the box, $b_{w}$ and $b_{h}$ are the width and height of the box, $c_{x}$ and $c_{y}$ are the location of filter application and $t_{i}$ are predicted during regression.","metadata":{"_uuid":"d9757c26fd968912885c1dec5c11902d9f73a02d"}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n## 4. Model definition\nI refered to the official ResNet implementation in Tensorflow in terms of how to arange the code. ","metadata":{"_uuid":"19167dc0cd191b099b99ea63f2ce3bff02df52e4"}},{"cell_type":"markdown","source":"### fixed padding的好处\n详见https://github.com/tensorflow/tensorflow/issues/18213\n如果不用fixed padding，padding不同边缘的尺寸就不可控，导致对像素级别的任务如detection并不友好，而yolo就是detection\n### Batch norm and fixed padding\nIt's useful to define `batch_norm` function since the model uses batch norms with shared parameters heavily. Also, same as ResNet, Yolo uses convolution with fixed padding, which means that padding is defined only by the size of the kernel.","metadata":{"_uuid":"7ca3c0f3ded8338494841e94ef11aea2c48abb82"}},{"cell_type":"code","source":"def batch_norm(inputs, training, data_format):\n    \"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n    \"\"\"\n    对data_format == 'channels_first'的理解：\n    https://blog.csdn.net/It_BeeCoder/article/details/85250979\n    兼容性问题，与模型本身无关\n    这是一个无可奈何的问题，在如何表示一组彩色图片的问题上，Theano和TensorFlow发生了分歧， '日'模式，也即Theano模式会把100张RGB三通道的16×32（高为16宽为32）彩色图表示为下面这种形式（100,3,16,32），Caffe采取的也是这种方式。第0个维度是样本维，代表样本的数目，第1个维度是通道维，代表颜色通道数。后面两个就是高和宽了。这种theano风格的数据组织方法，称为“channels_first”，即通道维靠前。\n    而TensorFlow，的表达形式是（100,16,32,3），即把通道维放在了最后，这种数据组织方式称为“channels_last”。\n    \"\"\"\n    \"\"\"对scale=True的理解：\n    https://tensorflow.google.cn/api_docs/python/tf/keras/layers/BatchNormalization?hl=en\n    此处并没有设置gamma的其他参数，所以设置成scale=False效果应该不变\n    \"\"\"\n    return tf.layers.batch_normalization(\n        inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON,\n        scale=True, training=training)\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n    \"\"\"ResNet implementation of fixed padding.\n\n    Pads the input along the spatial dimensions independently of input size.\n\n    Args:\n        inputs: Tensor input to be padded.\n        kernel_size: The kernel to be used in the conv2d or max_pool2d.\n        data_format: The input format.\n    Returns:\n        A tensor with the same format as the input.\n    \"\"\"\n    pad_total = kernel_size - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n\n    if data_format == 'channels_first':\n        \"\"\"\n        对tf.pad的解释：函数签名:tf.pad(tensor, paddings, mode='CONSTANT', constant_values=0, name=None)\n        tensor是被处理的张量，padding规定了头尾各填补多少位数据\n        \"\"\"\n        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                        [pad_beg, pad_end],\n                                        [pad_beg, pad_end]])\n    else:\n        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                        [pad_beg, pad_end], [0, 0]])\n    return padded_inputs\n\n\"\"\"\nstrides如果等于1则用tensorflow官方的padding，反之则用fixed_padding\n\"\"\"\ndef conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1):\n    \"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n    if strides > 1:\n        inputs = fixed_padding(inputs, kernel_size, data_format)\n    \"\"\"\n    详见 https://tensorflow.google.cn/api_docs/python/tf/compat/v1/layers/Conv2D?hl=en\n    对参数strides的解释：strides取一个数的时候代表长宽相等，kernel_size类似\n    \"\"\"\n    return tf.layers.conv2d(\n        inputs=inputs, filters=filters, kernel_size=kernel_size,\n        strides=strides, padding=('SAME' if strides == 1 else 'VALID'),\n        use_bias=False, data_format=data_format)","metadata":{"_uuid":"80c2ae4a51e7bc745f50997234ed9b738315b19b","execution":{"iopub.status.busy":"2022-06-21T11:58:28.592295Z","iopub.execute_input":"2022-06-21T11:58:28.592848Z","iopub.status.idle":"2022-06-21T11:58:28.609687Z","shell.execute_reply.started":"2022-06-21T11:58:28.592787Z","shell.execute_reply":"2022-06-21T11:58:28.608695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ImageNet\n著名图像数据库\n### DarkNet-53\n三个输出，作用可能是多“角度”学习，也对应了yolo模型中给出的三个bbox,其结构如图：\n![](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic2.zhimg.com%2Fv2-96400f91141d50a6cc2cb9e0d8325741_b.jpg&refer=http%3A%2F%2Fpic2.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1658382255&t=3bff06a7e16f4e6c7d1ba5ff46518870)\n\n### DarkNet-53在整个网络中的位置\n参考Yolo网络结构图，DarkNet-53只负责提取抽象特征，数据经DarkNet-53处理后，才是对bbox和分类的具体操作\n![](https://img-blog.csdnimg.cn/2019040211084050.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70)\n### Avgpool,Connected和softmax\nhttps://blog.csdn.net/VictorHan01/article/details/97268204\n\n最后三层 Avgpool,Connected和softmax层是用来在ImageNet数据集上训练分类任务时使用的.当我们使用Darknet-53作为YOLOv3中提取图像特征的主干时,则不再使用最后三层.\n### 残差网络\n残差网络训练的是残差，可以减少深度网络中信息前向传递的损失\n###残差块\n![](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic2.zhimg.com%2Fv2-35ce300cb393e8c992d567557f2b64da_b.jpg&refer=http%3A%2F%2Fpic2.zhimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1658376606&t=d56598c89e8e36664474963aeaa882e3)\n\n### Feature extraction: Darknet-53\nFor feature extraction Yolo uses Darknet-53 neural net pretrained on ImageNet. Same as ResNet,  Darknet-53 has shortcut (residual) connections, which help information from earlier layers flow further. We omit the last 3 layers (Avgpool, Connected and Softmax) since we only need the features.","metadata":{"_uuid":"5361e186fecd4ab9c53ee37987d905f0917f18d4"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\"\"\"本函数在网络输入端添加一个残差块\"\"\"\ndef darknet53_residual_block(inputs, filters, training, data_format,\n                             strides=1):\n    \"\"\"Creates a residual block for Darknet.\"\"\"\n    \"\"\"本文中添加层采用迭代的形式，形如：input=conv2d(inputs,...)\"\"\"\n    shortcut = inputs\n    inputs = conv2d_fixed_padding(\n        inputs, filters=filters, kernel_size=1, strides=strides,\n        data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    \n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n    \n    inputs = conv2d_fixed_padding(\n        inputs, filters=2 * filters, kernel_size=3, strides=strides,\n        data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    \"\"\"tf.nn提供低级的神经网络相关功能\"\"\"\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs += shortcut\n    return inputs\n\n\ndef darknet53(inputs, training, data_format):\n    \"\"\"Creates Darknet53 model for feature extraction.\"\"\"\n    inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n    inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3,\n                                  strides=2, data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = darknet53_residual_block(inputs, filters=32, training=training,\n                                      data_format=data_format)\n\n    inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3,\n                                  strides=2, data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    for _ in range(2):\n        inputs = darknet53_residual_block(inputs, filters=64,\n                                          training=training,\n                                          data_format=data_format)\n\n    inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3,\n                                  strides=2, data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    for _ in range(8):\n        inputs = darknet53_residual_block(inputs, filters=128,\n                                          training=training,\n                                          data_format=data_format)\n\n    route1 = inputs\n\n    inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3,\n                                  strides=2, data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    for _ in range(8):\n        inputs = darknet53_residual_block(inputs, filters=256,\n                                          training=training,\n                                          data_format=data_format)\n\n    route2 = inputs\n\n    inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3,\n                                  strides=2, data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    for _ in range(4):\n        inputs = darknet53_residual_block(inputs, filters=512,\n                                          training=training,\n                                          data_format=data_format)\n\n    return route1, route2, inputs","metadata":{"_uuid":"d1fdc7ef8b92f0fdd60ff57911edf48cafe49d83","execution":{"iopub.status.busy":"2022-06-21T11:58:28.611417Z","iopub.execute_input":"2022-06-21T11:58:28.611996Z","iopub.status.idle":"2022-06-21T11:58:28.743665Z","shell.execute_reply.started":"2022-06-21T11:58:28.611934Z","shell.execute_reply":"2022-06-21T11:58:28.742749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convolution layers\nYolo has a large number of convolutional layers. It's useful to group them in blocks.\n### 卷积块\n此处代码，图，原文命名稍有区别，参考Yolo网络结构图，下面的代码就是图中右下角的Convolutional set\n![](https://img-blog.csdnimg.cn/2019040211084050.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70)","metadata":{"_uuid":"1bcefa6e17216948560c6c2551197abb87b0c131"}},{"cell_type":"code","source":"def yolo_convolution_block(inputs, filters, training, data_format):\n    \"\"\"Creates convolution operations layer used after Darknet.\"\"\"\n    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    route = inputs\n\n    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    return route, inputs","metadata":{"_uuid":"42c6af6b2a1cc052b69cbb768bfdfeed60e627a0","execution":{"iopub.status.busy":"2022-06-21T11:58:28.745012Z","iopub.execute_input":"2022-06-21T11:58:28.745539Z","iopub.status.idle":"2022-06-21T11:58:28.758295Z","shell.execute_reply.started":"2022-06-21T11:58:28.745488Z","shell.execute_reply":"2022-06-21T11:58:28.757103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Detection layers\nYolo has 3 detection layers, that detect on 3 different scales using respective anchors. For each cell in the feature map the detection layer predicts `n_anchors * (5 + n_classes)` values using 1x1 convolution. For each scale we have `n_anchors = 3`. `5 + n_classes` means that respectively to each of 3 anchors we are going to predict 4 coordinates of the box, its confidence score (the probability of containing an object) and class probabilities. \n###预测bbox和分类\n\n根据Yolo网络结构图，Convolutional set左侧的几个方框（即下面代码中的yolo_layer）开始预测bbox和分类。对每个锚框，预测了bbox的各种偏移，和分类的各种可能性。\n![](https://img-blog.csdnimg.cn/2019040211084050.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70)","metadata":{"_uuid":"b6f6e900de2d5cd3a94d362d639fdde58f1a1707"}},{"cell_type":"code","source":"def yolo_layer(inputs, n_classes, anchors, img_size, data_format):\n    \"\"\"Creates Yolo final detection layer.\n\n    Detects boxes with respect to anchors.\n\n    Args:\n        inputs: Tensor input.\n        n_classes: Number of labels.\n        anchors: A list of anchor sizes.\n        img_size: The input size of the model.\n        data_format: The input format.\n\n    Returns:\n        Tensor output.\n    \"\"\"\n    n_anchors = len(anchors)\n\n    inputs = tf.layers.conv2d(inputs, filters=n_anchors * (5 + n_classes),\n                              kernel_size=1, strides=1, use_bias=True,\n                              data_format=data_format)\n\n    shape = inputs.get_shape().as_list()\n    grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3]\n    if data_format == 'channels_first':\n        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n    inputs = tf.reshape(inputs, [-1, n_anchors * grid_shape[0] * grid_shape[1],\n                                 5 + n_classes])\n\n    strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1])\n\n    box_centers, box_shapes, confidence, classes = \\\n        tf.split(inputs, [2, 2, 1, n_classes], axis=-1)\n\n    x = tf.range(grid_shape[0], dtype=tf.float32)\n    y = tf.range(grid_shape[1], dtype=tf.float32)\n    x_offset, y_offset = tf.meshgrid(x, y)\n    x_offset = tf.reshape(x_offset, (-1, 1))\n    y_offset = tf.reshape(y_offset, (-1, 1))\n    \"\"\"https://tensorflow.google.cn/api_docs/python/tf/concat?hl=en\"\"\"\n    \"\"\"拼接张量\"\"\"\n    x_y_offset = tf.concat([x_offset, y_offset], axis=-1)\n    \"\"\"https://tensorflow.google.cn/api_docs/python/tf/tile?hl=en\"\"\"\n    \"\"\"平铺x_y_offset\"\"\"\n    x_y_offset = tf.tile(x_y_offset, [1, n_anchors])\n    x_y_offset = tf.reshape(x_y_offset, [1, -1, 2])\n    box_centers = tf.nn.sigmoid(box_centers)\n    box_centers = (box_centers + x_y_offset) * strides\n    anchors = tf.tile(anchors, [grid_shape[0] * grid_shape[1], 1])\n    box_shapes = tf.exp(box_shapes) * tf.to_float(anchors)\n\n    confidence = tf.nn.sigmoid(confidence)\n\n    classes = tf.nn.sigmoid(classes)\n\n    inputs = tf.concat([box_centers, box_shapes,\n                        confidence, classes], axis=-1)\n\n    return inputs","metadata":{"_uuid":"9aa8b0d0f0e297787fe809353107ae379d1c7d24","execution":{"iopub.status.busy":"2022-06-21T11:58:28.75992Z","iopub.execute_input":"2022-06-21T11:58:28.760448Z","iopub.status.idle":"2022-06-21T11:58:28.776857Z","shell.execute_reply.started":"2022-06-21T11:58:28.760396Z","shell.execute_reply":"2022-06-21T11:58:28.77604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 下采样与上采样\nhttps://blog.csdn.net/weixin_44451032/article/details/99974665\nhttps://www.geeksforgeeks.org/what-is-upsampling-in-matlab/\n下采样：解决数据分布不均衡的下采样的目的就从多数集中选出一部分数据与少数集重新组合成一个新的数据集，即下采样\n上采样：同上，增加样本\nYolo中上采样的目的是统一尺寸\n\n### Upsample layer\nIn order to concatenate with shortcut outputs from Darknet-53 before applying detection on a different scale, we are going to upsample the feature map using nearest neighbor interpolation.","metadata":{"_uuid":"fa025a0ba6b8656f12d94402d932f471b7e21e80"}},{"cell_type":"code","source":"def upsample(inputs, out_shape, data_format):\n    \"\"\"Upsamples to `out_shape` using nearest neighbor interpolation.\"\"\"\n    if data_format == 'channels_first':\n        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n        new_height = out_shape[3]\n        new_width = out_shape[2]\n    else:\n        new_height = out_shape[2]\n        new_width = out_shape[1]\n\n    inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width))\n\n    if data_format == 'channels_first':\n        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n\n    return inputs","metadata":{"scrolled":true,"_uuid":"31a240155d9f96edf06011a4b51d4cd0a999d24d","execution":{"iopub.status.busy":"2022-06-21T11:58:28.778161Z","iopub.execute_input":"2022-06-21T11:58:28.778606Z","iopub.status.idle":"2022-06-21T11:58:28.794564Z","shell.execute_reply.started":"2022-06-21T11:58:28.778548Z","shell.execute_reply":"2022-06-21T11:58:28.793384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Non-max suppression\nThe model is going to produce a lot of boxes, so we need a way to discard the boxes with low confidence scores. Also, to avoid having multiple boxes for one object, we will discard the boxes with high overlap as well using non-max suppresion for each class.\n###这一步算法简称为NMS\n用于bbox去重","metadata":{"_uuid":"49717e7dcca05da2c9131c67f303c7554ce7a708"}},{"cell_type":"code","source":"def build_boxes(inputs):\n    \"\"\"Computes top left and bottom right points of the boxes.\"\"\"\n    center_x, center_y, width, height, confidence, classes = \\\n        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n\n    top_left_x = center_x - width / 2\n    top_left_y = center_y - height / 2\n    bottom_right_x = center_x + width / 2\n    bottom_right_y = center_y + height / 2\n\n    boxes = tf.concat([top_left_x, top_left_y,\n                       bottom_right_x, bottom_right_y,\n                       confidence, classes], axis=-1)\n\n    return boxes\n\n\ndef non_max_suppression(inputs, n_classes, max_output_size, iou_threshold,\n                        confidence_threshold):\n    \"\"\"Performs non-max suppression separately for each class.\n\n    Args:\n        inputs: Tensor input.\n        n_classes: Number of classes.\n        max_output_size: Max number of boxes to be selected for each class.\n        iou_threshold: Threshold for the IOU.\n        confidence_threshold: Threshold for the confidence score.\n    Returns:\n        A list containing class-to-boxes dictionaries\n            for each sample in the batch.\n    \"\"\"\n    batch = tf.unstack(inputs)\n    print(\"here\")\n    print(batch)\n    boxes_dicts = []\n    for boxes in batch:\n        boxes = tf.boolean_mask(boxes, boxes[:, 4] > confidence_threshold)\n        classes = tf.argmax(boxes[:, 5:], axis=-1)\n        classes = tf.expand_dims(tf.to_float(classes), axis=-1)\n        boxes = tf.concat([boxes[:, :5], classes], axis=-1)\n\n        boxes_dict = dict()\n        for cls in range(n_classes):\n            mask = tf.equal(boxes[:, 5], cls)\n            mask_shape = mask.get_shape()\n            if mask_shape.ndims != 0:\n                class_boxes = tf.boolean_mask(boxes, mask)\n                boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes,\n                                                              [4, 1, -1],\n                                                              axis=-1)\n                boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1])\n                indices = tf.image.non_max_suppression(boxes_coords,\n                                                       boxes_conf_scores,\n                                                       max_output_size,\n                                                       iou_threshold)\n                class_boxes = tf.gather(class_boxes, indices)\n                boxes_dict[cls] = class_boxes[:, :5]\n\n        boxes_dicts.append(boxes_dict)\n\n    return boxes_dicts","metadata":{"_uuid":"a32f6e7a74477dc1fe77b99ddcf72a776fea3a92","execution":{"iopub.status.busy":"2022-06-21T11:58:28.795861Z","iopub.execute_input":"2022-06-21T11:58:28.796496Z","iopub.status.idle":"2022-06-21T11:58:28.812785Z","shell.execute_reply.started":"2022-06-21T11:58:28.796447Z","shell.execute_reply":"2022-06-21T11:58:28.811642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final model class\nFinally, let's define the model class using all of the layers described previously. ","metadata":{"_uuid":"46fde971771dab00f4061af6b0641159988fb608"}},{"cell_type":"code","source":"class Yolo_v3:\n    \"\"\"Yolo v3 model class.\"\"\"\n\n    def __init__(self, n_classes, model_size, max_output_size, iou_threshold,\n                 confidence_threshold, data_format=None):\n        \"\"\"Creates the model.\n\n        Args:\n            n_classes: Number of class labels.\n            model_size: The input size of the model.\n            max_output_size: Max number of boxes to be selected for each class.\n            iou_threshold: Threshold for the IOU.\n            confidence_threshold: Threshold for the confidence score.\n            data_format: The input format.\n\n        Returns:\n            None.\n        \"\"\"\n        if not data_format:\n            if tf.test.is_built_with_cuda():\n                data_format = 'channels_first'\n            else:\n                data_format = 'channels_last'\n\n        self.n_classes = n_classes\n        self.model_size = model_size\n        self.max_output_size = max_output_size\n        self.iou_threshold = iou_threshold\n        self.confidence_threshold = confidence_threshold\n        self.data_format = data_format\n\n    def __call__(self, inputs, training):\n        \"\"\"Add operations to detect boxes for a batch of input images.\n\n        Args:\n            inputs: A Tensor representing a batch of input images.\n            training: A boolean, whether to use in training or inference mode.\n\n        Returns:\n            A list containing class-to-boxes dictionaries\n                for each sample in the batch.\n        \"\"\"\n        with tf.variable_scope('yolo_v3_model'):\n            if self.data_format == 'channels_first':\n                inputs = tf.transpose(inputs, [0, 3, 1, 2])\n\n            inputs = inputs / 255\n\n            route1, route2, inputs = darknet53(inputs, training=training,\n                                               data_format=self.data_format)\n\n            route, inputs = yolo_convolution_block(\n                inputs, filters=512, training=training,\n                data_format=self.data_format)\n            \"\"\"yolo_layer层对应图上往左弯的那个箭头，出最终结果\"\"\"\n            detect1 = yolo_layer(inputs, n_classes=self.n_classes,\n                                 anchors=_ANCHORS[6:9],\n                                 img_size=self.model_size,\n                                 data_format=self.data_format)\n\n            inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1,\n                                          data_format=self.data_format)\n            \"\"\"加了BN层和leaky_relu，对张量维度没有影响\"\"\"\n            inputs = batch_norm(inputs, training=training,\n                                data_format=self.data_format)\n            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n            upsample_size = route2.get_shape().as_list()\n            inputs = upsample(inputs, out_shape=upsample_size,\n                              data_format=self.data_format)\n            axis = 1 if self.data_format == 'channels_first' else 3\n            \"\"\"合并三组中的两组特征用于下一层训练\"\"\"\n            inputs = tf.concat([inputs, route2], axis=axis)\n            route, inputs = yolo_convolution_block(\n                inputs, filters=256, training=training,\n                data_format=self.data_format)\n            detect2 = yolo_layer(inputs, n_classes=self.n_classes,\n                                 anchors=_ANCHORS[3:6],\n                                 img_size=self.model_size,\n                                 data_format=self.data_format)\n            inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1,\n                                          data_format=self.data_format)\n            inputs = batch_norm(inputs, training=training,\n                                data_format=self.data_format)\n            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n            upsample_size = route1.get_shape().as_list()\n            inputs = upsample(inputs, out_shape=upsample_size,\n                              data_format=self.data_format)\n            inputs = tf.concat([inputs, route1], axis=axis)\n            route, inputs = yolo_convolution_block(\n                inputs, filters=128, training=training,\n                data_format=self.data_format)\n            detect3 = yolo_layer(inputs, n_classes=self.n_classes,\n                                 anchors=_ANCHORS[0:3],\n                                 img_size=self.model_size,\n                                 data_format=self.data_format)\n\n            inputs = tf.concat([detect1, detect2, detect3], axis=1)\n            inputs = build_boxes(inputs)\n\n            boxes_dicts = non_max_suppression(\n                inputs, n_classes=self.n_classes,\n                max_output_size=self.max_output_size,\n                iou_threshold=self.iou_threshold,\n                confidence_threshold=self.confidence_threshold)\n\n            return boxes_dicts\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-06-21T11:58:28.81665Z","iopub.execute_input":"2022-06-21T11:58:28.818026Z","iopub.status.idle":"2022-06-21T11:58:28.849089Z","shell.execute_reply.started":"2022-06-21T11:58:28.817833Z","shell.execute_reply":"2022-06-21T11:58:28.847973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n## 5. Utility functions\nHere are some utility functions that will help us load images as NumPy arrays, load class names from the official file and draw the predicted boxes.\n### 系统功能\n画图，加载文件等","metadata":{"_uuid":"cddc02edf2dbbefe566666d557881faa8b59155e"}},{"cell_type":"code","source":"def load_images(img_names, model_size):\n    \"\"\"Loads images in a 4D array.\n\n    Args:\n        img_names: A list of images names.\n        model_size: The input size of the model.\n        data_format: A format for the array returned\n            ('channels_first' or 'channels_last').\n\n    Returns:\n        A 4D NumPy array.\n    \"\"\"\n    imgs = []\n\n    for img_name in img_names:\n        img = Image.open(img_name)\n        img = img.resize(size=model_size)\n        img = np.array(img, dtype=np.float32)\n        img = np.expand_dims(img, axis=0)\n        imgs.append(img)\n\n    imgs = np.concatenate(imgs)\n\n    return imgs\n\n\ndef load_class_names(file_name):\n    \"\"\"Returns a list of class names read from `file_name`.\"\"\"\n    with open(file_name, 'r') as f:\n        class_names = f.read().splitlines()\n    return class_names\n\n\ndef draw_boxes(img_names, boxes_dicts, class_names, model_size):\n    \"\"\"Draws detected boxes.\n\n    Args:\n        img_names: A list of input images names.\n        boxes_dict: A class-to-boxes dictionary.\n        class_names: A class names list.\n        model_size: The input size of the model.\n\n    Returns:\n        None.\n    \"\"\"\n    colors = ((np.array(color_palette(\"hls\", 80)) * 255)).astype(np.uint8)\n    for num, img_name, boxes_dict in zip(range(len(img_names)), img_names,\n                                         boxes_dicts):\n        img = Image.open(img_name)\n        draw = ImageDraw.Draw(img)\n        font = ImageFont.truetype(font='../input/futur.ttf',\n                                  size=(img.size[0] + img.size[1]) // 100)\n        resize_factor = \\\n            (img.size[0] / model_size[0], img.size[1] / model_size[1])\n        for cls in range(len(class_names)):\n            boxes = boxes_dict[cls]\n            if np.size(boxes) != 0:\n                color = colors[cls]\n                for box in boxes:\n                    xy, confidence = box[:4], box[4]\n                    xy = [xy[i] * resize_factor[i % 2] for i in range(4)]\n                    x0, y0 = xy[0], xy[1]\n                    thickness = (img.size[0] + img.size[1]) // 200\n                    for t in np.linspace(0, 1, thickness):\n                        xy[0], xy[1] = xy[0] + t, xy[1] + t\n                        xy[2], xy[3] = xy[2] - t, xy[3] - t\n                        draw.rectangle(xy, outline=tuple(color))\n                    text = '{} {:.1f}%'.format(class_names[cls],\n                                               confidence * 100)\n                    text_size = draw.textsize(text, font=font)\n                    draw.rectangle(\n                        [x0, y0 - text_size[1], x0 + text_size[0], y0],\n                        fill=tuple(color))\n                    draw.text((x0, y0 - text_size[1]), text, fill='black',\n                              font=font)\n\n        display(img)","metadata":{"_uuid":"b992d6d649fdcb4c279deee99b2f66362066646c","_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-21T11:58:28.850713Z","iopub.execute_input":"2022-06-21T11:58:28.851419Z","iopub.status.idle":"2022-06-21T11:58:28.87774Z","shell.execute_reply.started":"2022-06-21T11:58:28.851165Z","shell.execute_reply":"2022-06-21T11:58:28.876738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n## 6. Converting weights to Tensorflow format\nNow it's time to load the official weights. We are going to iterate through the file and gradually create `tf.assign` operations.","metadata":{"_uuid":"13f8a2f689aff3e5941e94343a2f060f9049917f"}},{"cell_type":"code","source":"def load_weights(variables, file_name):\n    \"\"\"Reshapes and loads official pretrained Yolo weights.\n\n    Args:\n        variables: A list of tf.Variable to be assigned.\n        file_name: A name of a file containing weights.\n\n    Returns:\n        A list of assign operations.\n    \"\"\"\n    with open(file_name, \"rb\") as f:\n        # Skip first 5 values containing irrelevant info\n        np.fromfile(f, dtype=np.int32, count=5)\n        weights = np.fromfile(f, dtype=np.float32)\n\n        assign_ops = []\n        ptr = 0\n\n        # Load weights for Darknet part.\n        # Each convolution layer has batch normalization.\n        for i in range(52):\n            conv_var = variables[5 * i]\n            gamma, beta, mean, variance = variables[5 * i + 1:5 * i + 5]\n            batch_norm_vars = [beta, gamma, mean, variance]\n\n            for var in batch_norm_vars:\n                shape = var.shape.as_list()\n                num_params = np.prod(shape)\n                var_weights = weights[ptr:ptr + num_params].reshape(shape)\n                ptr += num_params\n                assign_ops.append(tf.assign(var, var_weights))\n\n            shape = conv_var.shape.as_list()\n            num_params = np.prod(shape)\n            var_weights = weights[ptr:ptr + num_params].reshape(\n                (shape[3], shape[2], shape[0], shape[1]))\n            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n            ptr += num_params\n            assign_ops.append(tf.assign(conv_var, var_weights))\n\n        # Loading weights for Yolo part.\n        # 7th, 15th and 23rd convolution layer has biases and no batch norm.\n        ranges = [range(0, 6), range(6, 13), range(13, 20)]\n        unnormalized = [6, 13, 20]\n        for j in range(3):\n            for i in ranges[j]:\n                current = 52 * 5 + 5 * i + j * 2\n                conv_var = variables[current]\n                gamma, beta, mean, variance =  \\\n                    variables[current + 1:current + 5]\n                batch_norm_vars = [beta, gamma, mean, variance]\n\n                for var in batch_norm_vars:\n                    shape = var.shape.as_list()\n                    num_params = np.prod(shape)\n                    var_weights = weights[ptr:ptr + num_params].reshape(shape)\n                    ptr += num_params\n                    assign_ops.append(tf.assign(var, var_weights))\n\n                shape = conv_var.shape.as_list()\n                num_params = np.prod(shape)\n                var_weights = weights[ptr:ptr + num_params].reshape(\n                    (shape[3], shape[2], shape[0], shape[1]))\n                var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n                ptr += num_params\n                assign_ops.append(tf.assign(conv_var, var_weights))\n\n            bias = variables[52 * 5 + unnormalized[j] * 5 + j * 2 + 1]\n            shape = bias.shape.as_list()\n            num_params = np.prod(shape)\n            var_weights = weights[ptr:ptr + num_params].reshape(shape)\n            ptr += num_params\n            assign_ops.append(tf.assign(bias, var_weights))\n\n            conv_var = variables[52 * 5 + unnormalized[j] * 5 + j * 2]\n            shape = conv_var.shape.as_list()\n            num_params = np.prod(shape)\n            var_weights = weights[ptr:ptr + num_params].reshape(\n                (shape[3], shape[2], shape[0], shape[1]))\n            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n            ptr += num_params\n            assign_ops.append(tf.assign(conv_var, var_weights))\n\n    return assign_ops","metadata":{"_uuid":"24d4689103695a95a2634c4d688475ba2750fdea","execution":{"iopub.status.busy":"2022-06-21T11:58:28.880074Z","iopub.execute_input":"2022-06-21T11:58:28.880507Z","iopub.status.idle":"2022-06-21T11:58:28.904448Z","shell.execute_reply.started":"2022-06-21T11:58:28.880431Z","shell.execute_reply":"2022-06-21T11:58:28.903637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n## 7. Running the model\nNow we can run the model using some sample images.","metadata":{"_uuid":"f4ce260c2e3c4f0aad9e711796955586b5f0e674"}},{"cell_type":"markdown","source":"### Sample images","metadata":{"_uuid":"ed0422e95da34c20827fe85a31907fe7d8b3a648"}},{"cell_type":"code","source":"img_names = ['../input/dog.jpg', '../input/office.jpg']\nfor img in img_names: display(Image.open(img))","metadata":{"_uuid":"a4db589483f06d03a5053efb80a624b54517925f","scrolled":true,"execution":{"iopub.status.busy":"2022-06-21T11:58:28.906037Z","iopub.execute_input":"2022-06-21T11:58:28.906702Z","iopub.status.idle":"2022-06-21T11:58:32.97343Z","shell.execute_reply.started":"2022-06-21T11:58:28.906638Z","shell.execute_reply":"2022-06-21T11:58:32.9722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Detections\nTesting the model with IoU (Interception over Union ratio used in non-max suppression) threshold and confidence threshold both set to 0.5.","metadata":{"_uuid":"41bed33dcd8624518a94e3aca8f96a1891fad0c5"}},{"cell_type":"code","source":"batch_size = len(img_names)\nbatch = load_images(img_names, model_size=_MODEL_SIZE)\nclass_names = load_class_names('../input/coco.names')\nn_classes = len(class_names)\nmax_output_size = 10\niou_threshold = 0.5\nconfidence_threshold = 0.5\n\"\"\"构造model\"\"\"\nmodel = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE,\n                max_output_size=max_output_size,\n                iou_threshold=iou_threshold,\n                confidence_threshold=confidence_threshold)\ninputs = tf.placeholder(tf.float32, [batch_size, 416, 416, 3])\n\n\"\"\"\nINPUTS = tf.keras.Input(shape=( 416, 416, 3),dtype=tf.float32)\nOUTPUTS= model(INPUTS,training=True)\nMODEL=tf.keras.Model(INPUTS,OUTPUTS)\n#MODEL.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n\"\"\"\n\ndetections = model(inputs, training=False)\n\nmodel_vars = tf.global_variables(scope='yolo_v3_model')\nassign_ops = load_weights(model_vars, '../input/yolov3.weights')\n\nwith tf.Session() as sess:\n    sess.run(assign_ops)\n    detection_result = sess.run(detections, feed_dict={inputs: batch})\n    \ndraw_boxes(img_names, detection_result, class_names, _MODEL_SIZE)\n","metadata":{"_uuid":"40b4a8d68a58cc2c2fcccf0dc24345f92ae955c2","execution":{"iopub.status.busy":"2022-06-21T11:58:32.975305Z","iopub.execute_input":"2022-06-21T11:58:32.975595Z","iopub.status.idle":"2022-06-21T11:59:04.383871Z","shell.execute_reply.started":"2022-06-21T11:58:32.975541Z","shell.execute_reply":"2022-06-21T11:59:04.38233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n## 8. Video processing\nI also applied the same algorithm to video detections. The code is available on my [GitHub repo](https://github.com/heartkilla/yolo-v3). <br>\nHere is an example of applying Yolo to a video I found on YouTube. ([A Street Walk in Shinjuku, Tokyo, Japan](https://www.youtube.com/watch?v=kZ7caIK4RXI))\n![](https://github.com/heartkilla/yolo-v3/blob/master/data/detection_examples/detections.gif)","metadata":{"_uuid":"f257a5535003cce9569460ddfde51d9c489724a7"}},{"cell_type":"code","source":"\"\"\"\nfrom IPython.display import Image\nwith open('../input/detections.gif','rb') as f:\n    display(Image(data=f.read(), format='png'))\n\"\"\"","metadata":{"_uuid":"fb46fa422366237bac8dfe281950aa44e7a9c798","_kg_hide-output":false,"_kg_hide-input":true,"scrolled":true,"execution":{"iopub.status.busy":"2022-06-21T11:59:04.387066Z","iopub.execute_input":"2022-06-21T11:59:04.38748Z","iopub.status.idle":"2022-06-21T11:59:04.395718Z","shell.execute_reply.started":"2022-06-21T11:59:04.387406Z","shell.execute_reply":"2022-06-21T11:59:04.394696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n## 9. To-Do list\n* Training","metadata":{"_uuid":"c32818f198d3078121bcbf6b24b3ddde2d653a54","trusted":true}},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n## 10. Acknowledgements\n* [Yolo v3 official paper](https://arxiv.org/abs/1804.02767)\n* [A Tensorflow Slim implementation](https://github.com/mystic123/tensorflow-yolo-v3)\n* [ResNet official implementation](https://github.com/tensorflow/models/tree/master/official/resnet)\n* [DeviceHive video analysis repo](https://github.com/devicehive/devicehive-video-analysis)\n* [A Street Walk in Shinjuku, Tokyo, Japan](https://www.youtube.com/watch?v=kZ7caIK4RXI)\n\nSpecial thanks to [Paul]( https://www.kaggle.com/paultimothymooney) for posting this kernel in [Kaggle Data Notes Newsletter](https://www.kaggle.com/page/data-notes).\n","metadata":{"_uuid":"ec3a0a5a5f284fe92caa86cf89e58ec471cdb4e5","execution":{"iopub.status.busy":"2022-06-21T11:55:07.203935Z","iopub.execute_input":"2022-06-21T11:55:07.204582Z","iopub.status.idle":"2022-06-21T11:55:07.216273Z","shell.execute_reply.started":"2022-06-21T11:55:07.204491Z","shell.execute_reply":"2022-06-21T11:55:07.214186Z"}}}]}